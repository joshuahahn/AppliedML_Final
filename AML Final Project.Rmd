---
title: "AML Project"
author: "Wyatt King"
date: "2024-04-03"
output: html_document
---

```{r}
library(dplyr)
library(httr)
library(jsonlite)
library(parallel)
library(caret)
library(tidymodels)
library(tidyverse)
library(doParallel)
```

Here, we do some preliminary investigations to figure out insights about the dataset that can make the insights easier to retrieve. 
First, let's see what information we can first gather about our table:
(1) Number of features
(2) Number of observations
(3) Data types of columns
(4) Number of NA observations
(5) Zero-variance columns
(6) Distribution of our target feature (pymkttot: market assessed total value)


```{r}
csv <- read.table("/Users/wyatttheking/Desktop/Spring 2024/Machine Learning/Final Project/AML_Final_Project_Dataset.csv", sep=',', header = T)

print(dim(csv))
print(sapply(csv, class)) # sapply performs "class" on each column to retrieve its datatype.
print(sum(is.na(csv)))
print(colSums(is.na(csv)))
print(nearZeroVar(csv, saveMetrics = TRUE))

ggplot(data = csv) + geom_histogram(aes(x = pymkttot), fill = "tomato",
      alpha = 0.3, color = "black") + 
      labs(title = "Market Assessed Total Value, Raw Data", x = "Price (USD)", y = "COUNT") +
      theme_bw()
```

We can find several insights from these explorations:
(1) Number of features: 146
(2) Number of rows: 11115
(3) Data types: We mostly see integers, characters (strings), and logicals. However, some of these characters (strings) should actually be categorical features. We can address this, and also turn them into binary-encoded categorical features to make learning easier. 
(4) There are many columns with a lot of missing data. In fact, there are several columns that do not have a single data point (subident_reuc, ident, roll_selection, subident). We can prune these columns since they provide no meaningful insight for us.
(5) Similarly, there are quite a few columns with zero or near-zero variance. Of course, rows with no observations (as found in bullet point 4) will have zero variance. However, there are also some columns that do contain observations, but contain no meaningful variation within the columns. We will also prune these, since they add no value to our model.
(6) Our histogram has an extremely long tail to the right. This makes sense; we expect there to be many outliers towards the right end in NYC apartments, and a large concentration of buildings within the same price range. However, there is also a large number of pymkttot observations with 0 as the value. These will have to be removed from our observations, as we do not reasonably expect the price of a NYC apartment to be $0. 

Let's split our dataset into training & testing data, then address these insights one-by-one. 
```{r}
csv <- read.table("/Users/wyatttheking/Desktop/Spring 2024/Machine Learning/Final Project/AML_Final_Project_Dataset.csv", sep=',', header = T)
filtered_csv <- subset(csv, pymkttot != 0)
filtered_csv<-filtered_csv[-c(1, 2, 3)] # Remove PARID
```

First, we're going to remove nominal variables that have an absurdly large number of categories. While it would be nice to include them in our model, it is simply not practical to considering the computing power that it would take to do so.

```{r}
subset_nominal_columns <- function(data) {
  nominal_columns <- sapply(data, function(x) is.factor(x) || is.character(x))
  subset_data <- data[, nominal_columns]
  return(subset_data)
}

get_nom<-subset_nominal_columns(filtered_csv)

calculate_unique_entries <- function(data) {
  unique_counts <- sapply(data, function(x) length(unique(x)))
  return(unique_counts)
}

numb_in_cat<-calculate_unique_entries(get_nom)
print(numb_in_cat)
```

```{r}
# Owner. Isolate five largest property owners and make all else an "Other" 
unique_counts <- sapply(filtered_csv$owner, function(x) length(unique(x)))
freq_uniq_counts<-table(filtered_csv$owner)
gr_lw<- sort(freq_uniq_counts, decreasing = T)
top_six<-head(gr_lw) 
names(top_six) # We get the six property owners who own the highest number of props

rename_owner<-function(x){
  if(!(x %in% names(top_six)[-2])){ # We subset top_six to exclude entries w owner unavailable
    return("OTHER")
  }
  else{
    return(x)
  }
}

filtered_csv$owner<-sapply(filtered_csv$owner, rename_owner)
```

```{r}
# Street.
freq_uniq_counts<-table(filtered_csv$street_name)
gr_lw<- sort(freq_uniq_counts, decreasing = T)
top_six<-head(gr_lw)

rename_street<-function(x){
  if(!(x %in% names(top_six))){ # We subset top_six to exclude entries w owner unavailable
    return("OTHER")
  }
  else{
    return(x)
  }
}

filtered_csv$street_name<-sapply(filtered_csv$street_name, rename_street)
```

```{r}
# Housenum_low and housenum_hi, we will convert to being numeric
filtered_csv$housenum_lo<-as.numeric(filtered_csv$housenum_lo)
filtered_csv$housenum_hi<- as.numeric(filtered_csv$housenum_hi)
```

For building class, we will reduce the number of categories based on what type of dwelling each property is (as indicated by the letter).

```{r}
red_bldg_class<-function(y){
  split<- strsplit(y, "[0-9]")[[1]]
  return(split)
}

filtered_csv$bldg_class<-sapply(filtered_csv$bldg_class, red_bldg_class)
```

For date of the most recent apportionment, we switch from using the exact date to using the year, which we then turn into being a numeric variable.

```{r}
filtered_csv$appt_date<-as.numeric(substr(filtered_csv$appt_date, 1, 4))
```

In our data, there is no consistent way that properties label their apartment numbers, especially considering how our data includes many different types of properties. Consequently, the "apartment number" feature does not necessarily measure the same thing between properties. As a result, we're going to remove it from our data.

```{r}
filtered_csv<-filtered_csv[!(colnames(filtered_csv) %in% "aptno")]
```

Next, we're going to remove columns with a high concentration of NA values. 

```{r}
identify_na_columns <- function(data, threshold = 0.9) {
  na_counts <- colMeans(is.na(data))
  na_columns <- names(na_counts[na_counts > threshold])
  return(na_columns)
}

too_many_nas<- identify_na_columns(filtered_csv)
filtered_csv<-filtered_csv[!(colnames(filtered_csv) %in% c(too_many_nas))]
```

Next, we're going to remove columns that are very highly correlated with our response. We do so because these variables are pretty much the same measure as PYMKTTOT, so it would be akin to cheating if we included them in our model.

```{r}
correlate_with_all <- function(df, feature) {
  numeric_df <- df[sapply(df, is.numeric)]
  correlation <- cor(numeric_df[, feature], numeric_df, use = "pairwise.complete.obs")
  correlation<- as.data.frame(t(data.frame(correlation^2)))
  correlation$variable<-row.names(correlation) 
  return(correlation)
}

correlation_df<-correlate_with_all(filtered_csv, "pymkttot")
isolate_high_corr<- correlation_df[correlation_df$V1 >= .8 & correlation_df$V1 != 1,]

high_corr_remove<-na.omit(isolate_high_corr$variable)
```

```{r}
filtered_csv<-filtered_csv[!(colnames(filtered_csv) %in% high_corr_remove)]
```

```{r}
set.seed(1)
split <- initial_split(filtered_csv, prop = 0.70)
raw_train <- training(split)
raw_test <- testing(split)

blueprint <- recipe(pymkttot ~ ., data = raw_train) %>%
             step_string2factor(all_nominal_predictors()) %>%
             step_log(all_outcomes()) %>%
             step_nzv(all_predictors()) %>%
             step_impute_knn(all_predictors()) %>%
             step_center(all_numeric_predictors()) %>%
             step_scale(all_numeric_predictors()) %>%
             step_pca(all_numeric_predictors()) %>%
             step_dummy(all_nominal())
```

```{r}
set.seed(1)
blueprint_prep <- prep(blueprint, training = raw_train)
transformed_train<- bake(blueprint_prep, new_data = raw_train)
transformed_test<- bake(blueprint_prep, new_data = raw_test)
```

## Building our Models

### Random Forest

```{r}
library(randomForest) 
```

```{r}
initial_rf <- randomForest(pymkttot ~., data = transformed_train)
plot(initial_rf)
```

```{r}
set.seed(1)
resample <- trainControl(method = "cv", number = 5)

hyper_grid <- expand.grid(mtry = c(30, 40, 50, 60, 70),
                          splitrule = c("variance", "extratrees"),
                          min.node.size = c(3, 5, 7))

## Tuning Hyperparameters
set.seed(1)
rf_fit <- train(pymkttot ~ .,
                data = transformed_train, 
                method = "ranger", 
                trControl = resample, 
                tuneGrid = hyper_grid,
                metric = "RMSE",
                num.trees = 300) 
```

```{r}
ggplot(rf_fit)
# heatmap
trellis.par.set(caretTheme()) # optional
plot(rf_fit, metric = "RMSE", plotType = "level")
```

```{r}
results_df<-rf_fit$results
min_rmse<-results_df[results_df$RMSE== min(results_df$RMSE),]
print(min_rmse)
```

```{r}
fitControl_final <- trainControl(method = "none")
RF_final <- train(pymkttot ~., 
                  data = transformed_train,
                  method = "ranger",
                  trControl = fitControl_final,
                  metric = "RMSE",
                  tuneGrid = data.frame(mtry = min_rmse$mtry,
                                        min.node.size = min_rmse$min.node.size,
                                        splitrule = min_rmse$splitrule),
                  num.trees = 300)
```

```{r}
calculate_rmse <- function(actual, predicted) {
  squared_errors <- (actual - predicted) ^ 2
  mean_squared_error <- mean(squared_errors)
  rmse <- sqrt(mean_squared_error)
  return(rmse)
}
```

```{r}
RF_pred_train <- predict(RF_final, newdata = transformed_train)
plot(transformed_train$pymkttot, RF_pred_train)
train_rmse<-calculate_rmse(transformed_train$pymkttot, RF_pred_train)
cat("Our RMSE for our training data set is about ", round(train_rmse, 3), ".", sep="")

RF_pred_test <- predict(RF_final, newdata = transformed_test)
plot(transformed_test$pymkttot, RF_pred_test)
test_rmse<-calculate_rmse(transformed_test$pymkttot, RF_pred_test)
cat("Our RMSE for our testing data set is about ", round(test_rmse, 3), ".", sep="")
```

### XGBoost

```{r}
library(xgboost)
```

```{r}
set.seed(1)
resample <- trainControl(method = "cv", number = 5)
hyper_grid <- expand.grid(nrounds = c(100, 200, 300),  # number of trees 
                       max_depth = c(4, 6, 8, 10),     # maximum depth of a tree 
                       eta = c(0.05, 0.1, 0.2, 0.3),   # controls the learning rate
                       min_child_weight = c(5, 10, 15), # minimum sum of instance weight needed in a child node
                       subsample = c(0.4, 0.6),   # subsample ratio of the training instances
                       gamma = 0,   # minimum loss reduction required to make a further partition
                       colsample_bytree = 1) 
```

```{r}
set.seed(1)
job::job({
  xg_fit <- train(pymkttot ~ .,
                data = transformed_train, 
                method = "xgbTree", 
                trControl = resample, 
                tuneGrid = hyper_grid,
                metric = "RMSE",
                verbosity=0)
})
```

```{r}
plot(xg_fit)
```

```{r}
xg_df<-xg_fit$results[xg_fit$results$RMSE== min(xg_fit$results$RMSE),]
```

```{r}
fitControl_final <- trainControl(method = "none")
XG_final <- train(pymkttot ~., 
                  data = transformed_train,
                  method = "xgbTree",
                  trControl = fitControl_final,
                  metric = "RMSE",
                  tuneGrid = data.frame(eta=xg_df$eta,
                                        max_depth=xg_df$max_depth,
                                        gamma= xg_df$gamma,
                                        colsample_bytree= xg_df$colsample_bytree,
                                        min_child_weight= xg_df$min_child_weight,
                                        subsample= xg_df$subsample,
                                        nrounds=xg_df$nrounds),
                  verbosity=0)
```

```{r}
xg_pred_train <- predict(XG_final, newdata = transformed_train)
plot(transformed_train$pymkttot, xg_pred_train, main="XGBoost Model: Training Set", xlab= "Actual PYMKTTOT", ylab="Predicted PYMKTTOT")
train_rmse<-calculate_rmse(transformed_train$pymkttot, xg_pred_train)
cat("Our RMSE for our training data set is about ", round(train_rmse, 3), ".", sep="")


xg_pred_test <- predict(XG_final, newdata = transformed_test)
plot(transformed_test$pymkttot, xg_pred_test, main="XGBoost Model: Testing Set", xlab= "Actual PYMKTTOT", ylab="Predicted PYMKTTOT")
test_rmse<-calculate_rmse(transformed_test$pymkttot, xg_pred_test)
cat("Our RMSE for our testing data set is about ", round(test_rmse, 3), ".", sep="")
```

### SVM

```{r}
resample <- trainControl(method = "cv",
                         number = 5,
                         summaryFunction = defaultSummary)


SVMGrid_Lin <- expand.grid(C = c(0.01, 0.1, 1, 10)) 
SVMGrid_Poly <- expand.grid(C = c(0.01, 0.1, 1, 10),
                            degree = c(2, 3),
                            scale = 1)
SVMGrid_RBF <- expand.grid (sigma = c(0.01, 0.1, 1, 10),
                            C = c(0.01, 0.1, 1, 10))
```

```{r}
set.seed(1)
job::job({
  SVM_Linear <- train(pymkttot ~ ., 
                    data = transformed_train,
                    method = "svmLinear",
                    trControl = resample,
                    verbose = FALSE,
                    tuneGrid = SVMGrid_Lin,
                    metric = "RMSE")
})
```

```{r}
set.seed(1)
job::job({
  SVM_Poly <- train(pymkttot ~ ., 
                    data = transformed_train,
                    method = "svmPoly",
                    trControl = resample,
                    verbose = FALSE,
                    tuneGrid = SVMGrid_Poly,
                    metric = "RMSE")
})
```

```{r}
job::job({
  SVM_RBF <- train(pymkttot ~ ., 
                 data = transformed_train,
                 method = "svmRadial",
                 trControl = resample,
                 verbose = FALSE,
                 tuneGrid = SVMGrid_RBF,
                 metric = "RMSE")
})
```

```{r}
lin_results<-SVM_Linear$results
min_rmse1<-lin_results[lin_results$RMSE== min(lin_results$RMSE),]
  
poly_results<-SVM_Poly$results
min_rmse2<-poly_results[poly_results$RMSE== min(poly_results$RMSE),]

rbf_results<- SVM_RBF$results
min_rmse3<-rbf_results[rbf_results$RMSE== min (rbf_results$RMSE),]

l1<-list(min_rmse1, min_rmse2, min_rmse3)
print(l1)
```

```{r}
fitControl_final <- trainControl(method = "none")
svm_final <- train(pymkttot ~., 
                  data = transformed_train,
                  method = "svmRadial",
                  trControl = resample,
                  verbose = FALSE,
                  tuneGrid = data.frame(
                    sigma= min_rmse3$sigma,
                    C= min_rmse3$C
                  ),
                  metric = "RMSE")
```

```{r}
svm_pred_train <- predict(svm_final, newdata = transformed_train)
plot(transformed_train$pymkttot, svm_pred_train, main="SVM Model: Training Set", xlab= "Actual PYMKTTOT", ylab="Predicted PYMKTTOT")
train_rmse<-calculate_rmse(transformed_train$pymkttot, svm_pred_train)
cat("Our RMSE for our training data set is about ", round(train_rmse, 3), ".", sep="")

svm_pred_test <- predict(svm_final, newdata = transformed_test)
plot(transformed_test$pymkttot, svm_pred_test, main="SVM Model: Testing Set", xlab= "Actual PYMKTTOT", ylab="Predicted PYMKTTOT")
test_rmse<-calculate_rmse(transformed_test$pymkttot, svm_pred_test)
cat("Our RMSE for our testing data set is about ", round(test_rmse, 3), ".", sep="")
```









